---
title : "Build Amazon Bedrock Prompt Management Prompts"
weight : 10
---

## Section 1
## Build the Prompts

# Creating Amazon Bedrock Prompt Management Prompts

Amazon Bedrock provides the ability to create and save your own prompts using Prompt Management. This feature allows you to save time by applying the same prompt to different workflows. When creating a prompt, you can:

- Select a model to run inference on it
- Modify the inference parameters
- Include variables for flexibility in different use cases

## Workflow for Using Prompt Management

1. Create a prompt in Prompt Management for reuse across different use cases.
2. Choose a model for inference and modify configurations as needed.
3. Test the prompt with variable values and compare variants.

## Creating a Prompt

![AWS Management Console](/static/Module1Images/AWSConsole.png).

1. Sign in to the AWS Console
2. Open the Amazon Bedrock console ![Amazon Bedrock Console](/static/Module1Images/OpenAmazonBedrockConsole.png).
3. Select **Prompt Management** from the left navigation pane 

![AWS Management Console](/static/Module1Images/SelectPromptManagement.png).

4. Click **Create prompt** 

![Create Prompt Button](/static/Module1Images/CreatePromptButton.png).

5. Set the following values:
   - **Name**: `TLC302-Create-Transcript`
   - **Description**: `This prompt creates a synthetic contact center transcript based on the user input of the {{issue}}`
   ![Name and Description](/static/Module1Images/CreatePromptNameDescription.png)

6. Click the **Create button**.

## Configuring Your Prompt

1. In Prompt Management, you will see a green banner at the top of the page indicating your prompt has been successfully built.  Default configuration text is populated in the Prompt section.  We will update this text to be the actual prompt we need for the Workshop use case.

![PromptCreated](/static/Module1Images/PromptCreated.png)

2. In the Prompt pane, replace the default text of:

`This is my first text prompt. Please summarize our conversation on {{topic}}`

with this text:

:::code{showCopyAction=true showLineNumbers=true language=text}
'Create a contact center transcript for issue of {{issue}}'
:::

   **Note:** We use double curly braces `{{}}` to include the variable `{{issue}}`.

![Prompt And Model Selection](/static/Module1Images/ReplacePromptTextAndSelectModel.png)

3. In the Configurations pane:
   - Choose the **Model** for running inference

   ![Select Model](/static/Module1Images/SelectModel.png)

   - Set the **Inference parameters** as required

   ![Set Model Parameters](/static/Module1Images/CreatePromptNew.png)

4. Click **Save draft** to save your prompt.  You will see a green banner at the top of the screen indicating the prompt was successfully saved.

Now, we can test the prompt by entering the issue of "bill shock" in the Variable value field under Test Variables.  The Test window on the righthand side of the screen will show the prompt with the {{issue}} variable as "bill shock" and the resulting contact center transcript generated by the model.

![Test Prompt 1](/static/Module1Images/TestPrompt1.png)

### About Prompt Variables

- Each variable appears in the Test variables section.
- You can replace these variables with actual values when testing or configuring the prompt in a prompt flow.

## Creating the Second Prompt

Let's repeat the same steps using the following details for the second prompt:

### Prompt Details

- **Name**: `TLC302-Analyze-Transcript`
- **Description**: `This prompt analyzes the previously generated transcript to parse for customer journey sub-events to report to the Connected Customer Journey tracking API`

Once the prompt is created, edit the prompt text and configure the model settings as shown here:

![Configure Prompt 2](/static/Module1Images/ConfigurePrompt2.png)

> **Important**: Ensure that the model and inference parameters match exactly with those used in the image above.

Prompt text to paste in Prompt pane:

<div style="position: relative;">
<button style="position: absolute; top: 10px; right: 10px; padding: 5px;" onclick="navigator.clipboard.writeText(this.parentElement.querySelector('pre').textContent)">ðŸ“‹</button>

:::code{showCopyAction=true showLineNumbers=true language=text}
You are an expert at analyzing contact center transcripts and providing a Summary, Issues, Actions, Outcomes all in JSON format.

 Use {{transcript}} to derive these outputs. 
 
 The JSON format for output must contain random values for "call_id": "202305", "date": "2023-05-15", "duration": "00:07:23", "agent_id": "AG-5678", "customer_id": "1234" "issues": [ { "id": "ISS-001" fields and be in this precise format: 
 
 json { "transcript_analysis": { "call_id": "CC-2023-05-15-001", "date": "2023-05-15", "duration": "00:12:34", "agent_id": "AG-1234", "customer_id": "CUS-5678", "summary": "Customer called to report issues with their internet service and requested a technician visit. The agent troubleshooted the problem, scheduled a technician, and provided a service credit.", "issues": [ { "id": "ISS-001", "description": "Intermittent internet connection", "severity": "High" }, { "id": "ISS-002", "description": "Wi-Fi signal strength weak in certain areas of the house", "severity": "Medium" } ], "actions": [ { "id": "ACT-001", "description": "Performed remote diagnostics on the modem", "result": "No issues detected with the modem" }, { "id": "ACT-002", "description": "Guided customer through router reset process", "result": "Temporary improvement, but issue persisted" }, { "id": "ACT-003", "description": "Scheduled technician visit", "result": "Appointment set for 2023-05-17, 2-4 PM" }, { "id": "ACT-004", "description": "Applied service credit to customer account", "result": "$20 credit applied for the inconvenience" } ], "outcomes": [ { "id": "OUT-001", "description": "Technician visit scheduled to resolve connection issues", "status": "Pending" }, { "id": "OUT-002", "description": "Customer satisfied with service credit and prompt scheduling", "status": "Resolved" }, { "id": "OUT-003", "description": "Follow-up call to be made after technician visit", "status": "Scheduled" } ] } } 
 
 Return only the JSON object, no comments, no explanations, no introductions, no headers or footers indicating JSON The random values for "issues": [ { "id": "ISS-001" shall range from ISS-900 to ISS 90000
:::
</div>

To test the second prompt, open the TLC302-Create-Transcript prompt (the first one we created) and select the Open in Prompt Builder button

![Open in Prompt Builder](/static/Module1Images/OpenPromptBuilder.png)

Enter "bill shock" in the variable value field as we did in the original test, then press the Run button.  A transcript will be generated.  Copy the transcript.  We will use this to test the TLC302-Analyze-Prompt (second prompt) we created.

Navigate to the TLC302-Analyze-Prompt and once again select the Open in Prompt Builder button.  Paste the transcript into the Variable value in the Test variables pane, then press the Run button.  

![Second Prompt Test](/static/Module1Images/SecondPromptTest.png)

You will see a JSON value created as shown in the image.  Here is a sample JSON created by the prompt as a result of analyzing the input variable of {{transcript}} we generated in the first prompt:

:::code{showCopyAction=true showLineNumbers=true language=json}
{
  "transcript_analysis": {
    "call_id": "202305",
    "date": "2023-05-15",
    "duration": "00:07:23",
    "agent_id": "AG-5678",
    "customer_id": "1234",
    "summary": "The customer called to report high charges on their recent phone bill, which were due to unauthorized international calls made from their account. The agent investigated the issue, identified the fraudulent activity, and placed a credit on the customer's account to reverse the charges. The agent also opened an investigation and arranged to send the customer a new SIM card to prevent further unauthorized access.",
    "issues": [
      {
        "id": "ISS-12345",
        "description": "Unauthorized international calls on the customer's account",
        "severity": "High"
      }
    ],
    "actions": [
      {
        "id": "ACT-001",
        "description": "Reviewed the account details and call history",
        "result": "Identified a significant number of international calls to a European number"
      },
      {
        "id": "ACT-002",
        "description": "Placed a credit on the customer's account to reverse the international call charges",
        "result": "Credit applied to the customer's account"
      },
      {
        "id": "ACT-003",
        "description": "Opened an investigation into the fraudulent activity",
        "result": "Investigation initiated"
      },
      {
        "id": "ACT-004",
        "description": "Arranged to send the customer a new SIM card",
        "result": "New SIM card to be sent to the customer"
      }
    ],
    "outcomes": [
      {
        "id": "OUT-001",
        "description": "Unauthorized charges reversed on the customer's account",
        "status": "Resolved"
      },
      {
        "id": "OUT-002",
        "description": "Investigation into the fraudulent activity initiated",
        "status": "In Progress"
      },
      {
        "id": "OUT-003",
        "description": "New SIM card to be sent to the customer to prevent further unauthorized access",
        "status": "Pending"
      }
    ]
  }
}
:::

The **TLC302-AnalyzeTranscript** prompt identified the **Issue, Actions, and Outcomes** based on the content of the transcript.  This information was then formatted by the model to match the needed JSON schema to send to the Connected Customer Journey solution, which will be executed when we build the AWS Lambda and Amazon Bedrock Prompt Flow next in this module.

By following the steps above, you will have created two prompts in Amazon Bedrock Prompt Management, tested them successfully, and are ready to use in the next Workshop activities.
